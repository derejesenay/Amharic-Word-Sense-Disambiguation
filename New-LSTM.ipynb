{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "mounted-sunday",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorboard as tb\n",
    "import pickle\n",
    "import datetime\n",
    "import math\n",
    "import json\n",
    "import operator\n",
    "import pickle\n",
    "import sklearn\n",
    "import scrapy\n",
    "import codecs\n",
    "import contrib\n",
    "import string \n",
    "import nltk\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Activation, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.probability import FreqDist\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.cluster  import k_means\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.layers import Bidirectional, Embedding, Dropout, Dense, LSTM\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing import text\n",
    "from keras.models import load_model\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fantastic-california",
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = pd.read_csv (r'C:/Users/HANU-TECH/Desktop/progress-report/awsd.csv')\n",
    "#print (datas)\n",
    "dataset = datas.values.tolist()\n",
    "print(dataset[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tired-safety",
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_process(data):\n",
    "    panctuations='''።!)(-[]{};:'\\,\\<>./?@#$%^&*_~፤፣.-”“፠፥፦፧፨፡፡'''\n",
    "    amharic_numbers=['፩','፫','፪','፬','፭','፮','፯','፰','፱''፲','፳','፴','፵','፶','፷','፸',\n",
    "   '፹','፺','፻','፼','0','1','2','3','4','5','6','7','8','9']\n",
    "    num_dataset=len(data)\n",
    "    index=0\n",
    "    while index<=(num_dataset-1):#inside this loop tokenization, stop-word removal and stemming functions wiill be implmentedsen=string_data[0][0]\n",
    "        sen=data[index][0]\n",
    "        sen_list=sen.split()\n",
    "        #print(sen_list)\n",
    "        def remove_panctuations():\n",
    "            index_p=0\n",
    "            size=len(sen_list)\n",
    "            for word in sen_list:\n",
    "                first_char=word[0]\n",
    "                if first_char in panctuations and index_p<=(size-1):\n",
    "                    temp=word[1:]\n",
    "                    sen_list[index_p]=temp\n",
    "                    #index_p=index_p+1\n",
    "                elif index_p<=(size-1):\n",
    "                    sen_list[index_p]=word\n",
    "                    #index_p=index_p+1\n",
    "                else:\n",
    "                    break\n",
    "                index_p=index_p+1\n",
    "            index_p=0\n",
    "            for x in sen_list:# for removing panctuations from the end of each token.\n",
    "                if len(x)>1 and x[-1] in panctuations:\n",
    "                    temp=x[:-1]\n",
    "                    sen_list[index_p]=temp\n",
    "                    #index_p=index_p+1\n",
    "                else:\n",
    "                    sen_list[index_p]=x\n",
    "                    #index_p=index_p+1\n",
    "                index_p=index_p+1\n",
    "        remove_panctuations()\n",
    "        #print(\"the list of the sentnces: \",sen_list)\n",
    "        # here is a code which changes the list into text\n",
    "        sen_text=\"\"\n",
    "        index_np=0\n",
    "        size_np=len(sen_list)\n",
    "        while index_np<=(size_np-1):\n",
    "            sen_text=sen_text+\" \"+sen_list[index_np]\n",
    "            index_np=index_np+1\n",
    "        #print(\"the text of the sentences is: \", sen_text)\n",
    "        normalized_sen=\"\"\n",
    "        #Below a lsit for Alphabet normalization is listed down. \n",
    "        ha_type_replacment=['ሀ','ሁ','ሂ','ሄ', 'ህ', 'ሆ']\n",
    "        sa_type_replacment=['ሠ','ሡ','ሢ','ሣ','ሤ','ሥ','ሦ']\n",
    "        a_type_replacement=['አ','ኡ','ኢ','ኤ','እ','ኦ ']\n",
    "        wa_type_replacement=['ወ']\n",
    "        te_type_replacement=['ፀ','ፁ','ፂ','ፃ','ፄ','ፅ','ፆ']\n",
    "        ሀ_collection=['ሀ','ሃ','ሐ','ሓ','ኀ','ኃ']\n",
    "        ሁ_collection=['ሁ','ሑ','ኁ']\n",
    "        ሂ_collection=['ሂ','ሒ','ኂ']\n",
    "        ሄ_collection=['ሄ','ሔ','ኄ']\n",
    "        ህ_collection=['ህ','ሕ','ኅ']\n",
    "        ሆ_collection=['ሆ','ሖ','ኆ']\n",
    "        ሠ_collection=['ሠ','ሰ']\n",
    "        ሡ_collection=['ሡ','ሱ']\n",
    "        ሢ_collection=['ሢ','ሲ']\n",
    "        ሣ_collection=['ሣ','ሳ']\n",
    "        ሤ_collection=['ሤ','ሴ']\n",
    "        ሥ_collection=['ሥ','ስ']\n",
    "        ሦ_collection=['ሦ','ሶ']\n",
    "        አ_collection=['አ','ኣ','ዐ','ዓ']\n",
    "        ኡ_collection=['ኡ','ዑ']\n",
    "        ኢ_collection=['ኢ','ዒ']\n",
    "        ኤ_collection=['ኤ','ዔ']\n",
    "        እ_collection=['እ','ዕ']\n",
    "        ኦ_collection=['ኦ','ዖ']\n",
    "        ወ_collection=['ዎ']\n",
    "        ፀ_collection=['ጸ']\n",
    "        ፁ_collection=['ጹ']\n",
    "        ፂ_collection=['ጺ']\n",
    "        ፃ_collection=['ጻ']\n",
    "        ፄ_collection=['ጼ']\n",
    "        ፅ_collection=['ጽ']\n",
    "        ፆ_collection=['ጾ']\n",
    "        normalized_sen=\"\"\n",
    "        size=len(sen_text)\n",
    "        index_n=0\n",
    "        #here is a loop for character normalization\n",
    "        while index_n<=(size-1):\n",
    "            char=sen_text[index_n]\n",
    "            if char in ሀ_collection:\n",
    "                char_replace=ha_type_replacment[0]\n",
    "                normalized_sen=normalized_sen+char_replace\n",
    "                index_n=index_n+1\n",
    "            elif char in ሁ_collection:\n",
    "                char_replace=ha_type_replacment[1]\n",
    "                normalized_sen=normalized_sen+char_replace\n",
    "                index_n=index_n+1\n",
    "            elif char in ሂ_collection:\n",
    "                char_replace=ha_type_replacment[2]\n",
    "                normalized_sen=normalized_sen+char_replace\n",
    "                index_n=index_n+1\n",
    "            elif char in ሄ_collection :\n",
    "                char_replace=ha_type_replacment[3]\n",
    "                normalized_sen=normalized_sen+char_replace\n",
    "                index_n=index_n+1\n",
    "            elif char in ህ_collection:\n",
    "                char_replace=ha_type_replacment[4]\n",
    "                normalized_sen=normalized_sen+char_replace\n",
    "                index_n=index_n+1\n",
    "            elif char in ሆ_collection:\n",
    "                char_replace=ha_type_replacment[5]\n",
    "                normalized_sen=normalized_sen+char_replace\n",
    "                index_n=index_n+1\n",
    "            elif char in ሠ_collection:\n",
    "                char_replace=sa_type_replacment[0]\n",
    "                normalized_sen=normalized_sen+char_replace\n",
    "                index_n=index_n+1\n",
    "            elif char in ሡ_collection:\n",
    "                char_replace=sa_type_replacment[1]\n",
    "                normalized_sen=normalized_sen+char_replace\n",
    "                index_n=index_n+1\n",
    "            elif char in ሢ_collection:\n",
    "                char_replace=sa_type_replacment[2]\n",
    "                normalized_sen=normalized_sen+char_replace\n",
    "                index_n=index_n+1\n",
    "            elif char in ሣ_collection:\n",
    "                char_replace=sa_type_replacment[3]\n",
    "                normalized_sen=normalized_sen+char_replace\n",
    "                index_n=index_n+1\n",
    "            elif char in ሤ_collection:\n",
    "                char_replace=sa_type_replacment[4]\n",
    "                normalized_sen=normalized_sen+char_replace\n",
    "                index_n=index_n+1\n",
    "            elif char in ሥ_collection:\n",
    "                char_replace=sa_type_replacment[5]\n",
    "                normalized_sen=normalized_sen+char_replace\n",
    "                index_n=index_n+1\n",
    "            elif char in ሦ_collection:\n",
    "                char_replace=sa_type_replacment[6]\n",
    "                normalized_sen=normalized_sen+char_replace\n",
    "                index_n=index_n+1\n",
    "            elif char in አ_collection:\n",
    "                char_replace=a_type_replacement[0]\n",
    "                normalized_sen=normalized_sen+char_replace\n",
    "                index_n=index_n+1\n",
    "            elif char in ኡ_collection:\n",
    "                char_replace=a_type_replacement[1]\n",
    "                normalized_sen=normalized_sen+char_replace\n",
    "                index_n=index_n+1\n",
    "            elif char in ኢ_collection:\n",
    "                char_replace=a_type_replacement[2]\n",
    "                normalized_sen=normalized_sen+char_replace\n",
    "                index_n=index_n+1\n",
    "            elif char in ኤ_collection:\n",
    "                char_replace=a_type_replacement[3]\n",
    "                normalized_sen=normalized_sen+char_replace\n",
    "                index_n=index_n+1\n",
    "            elif char in እ_collection:\n",
    "                char_replace=a_type_replacement[4]\n",
    "                normalized_sen=normalized_sen+char_replace\n",
    "                index_n=index_n+1\n",
    "            elif char in ኦ_collection:\n",
    "                char_replace=a_type_replacement[5]\n",
    "                normalized_sen=normalized_sen+char_replace\n",
    "                index_n=index_n+1\n",
    "            elif char in ወ_collection:\n",
    "                char_replace=wa_type_replacement[0]\n",
    "                normalized_sen=normalized_sen+char_replace\n",
    "                index_n=index_n+1\n",
    "            elif char in ፀ_collection:\n",
    "                char_replace=te_type_replacement[0]\n",
    "                normalized_sen=normalized_sen+char_replace\n",
    "                index_n=index_n+1\n",
    "            elif char in ፁ_collection:\n",
    "                char_replace=te_type_replacement[1]\n",
    "                normalized_sen=normalized_sen+char_replace\n",
    "                index_n=index_n+1\n",
    "            elif char in ፂ_collection:\n",
    "                char_replace=te_type_replacement[2]\n",
    "                normalized_sen=normalized_sen+char_replace\n",
    "                index_n=index_n+1\n",
    "            elif char in ፃ_collection:\n",
    "                char_replace=te_type_replacement[3]\n",
    "                normalized_sen=normalized_sen+char_replace\n",
    "                index_n=index_n+1\n",
    "            elif char in ፄ_collection:\n",
    "                char_replace=te_type_replacement[4]\n",
    "                normalized_sen=normalized_sen+char_replace\n",
    "                index_n=index_n+1\n",
    "            elif char in ፅ_collection:\n",
    "                char_replace=te_type_replacement[5]\n",
    "                normalized_sen=normalized_sen+char_replace\n",
    "                index_n=index_n+1\n",
    "            elif char in ፆ_collection:\n",
    "                char_replace=te_type_replacement[6]\n",
    "                normalized_sen=normalized_sen+char_replace\n",
    "                index_n=index_n+1\n",
    "            elif index_n<=(size-1):\n",
    "                normalized_sen=normalized_sen+char\n",
    "                index_n=index_n+1\n",
    "            else:\n",
    "                break   \n",
    "        #print(\"the normalized sentences is: \", normalized_sen)\n",
    "        sen_list2=normalized_sen.split()\n",
    "        #print(sen_list2)\n",
    "        stopword=open('C:/Users/HANU-TECH/Desktop/progress-report/stop_word.txt',encoding='utf-8-sig',mode='r')\n",
    "        stopwords=stopword.read()\n",
    "        stopword_list=stopwords.split()\n",
    "        sen_non_stopword=[]\n",
    "        for terms in sen_list2:\n",
    "            if terms not in stopword_list:\n",
    "                sen_non_stopword.append(terms)\n",
    "        sen_nonstopword_text=\"\"\n",
    "        index_nonstop=0\n",
    "        size_nonstopword=len(sen_non_stopword)\n",
    "        while index_nonstop<=(size_nonstopword-1):\n",
    "            sen_nonstopword_text=sen_nonstopword_text+\" \"+sen_non_stopword[index_nonstop]\n",
    "            index_nonstop=index_nonstop+1\n",
    "        #print(sen_non_stopword)\n",
    "        data[index][0]=sen_nonstopword_text\n",
    "        index=index+1\n",
    "    return data\n",
    "processed=per_process(dataset)\n",
    "#print(processed)\n",
    "df=pd.DataFrame(processed) #convert list back to pandas dataframe\n",
    "df.shape\n",
    "#df_col_len = int(train_data['new'].str.encode(encoding='utf-8').str.len().max())\n",
    "\n",
    "df[0]\n",
    "train_data = df.rename(columns={0: 'sentences', 1: 'label', 2: 'relation'})\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interested-integer",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_t = pd.read_csv(r'C:/Users/HANU-TECH/Desktop/progress-report/test_data.csv')\n",
    "datam = data_t.values.tolist()\n",
    "#print(datam[0][0])\n",
    "processed=per_process(datam)\n",
    "#print(processed)\n",
    "df=pd.DataFrame(datam) #convert list back to pandas dataframe\n",
    "df.shape\n",
    "#df_col_len = int(train_data['new'].str.encode(encoding='utf-8').str.len().max())\n",
    "\n",
    "df[0]\n",
    "test_data = df.rename(columns={0: 'sentences', 1: 'label', 2: 'relation'})\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alert-canberra",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['sentences'].apply(lambda x:x.lower())\n",
    "#train_data['sentences'] = data['sentences'].apply(lambda x:re.sub('[A-Za-z0-9\\s]','',x)\n",
    "train_data['sentences'].head()                                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painful-giant",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = 500000, split = \" \")\n",
    "tokenizer.fit_on_texts(train_data['sentences'].values)\n",
    "x = tokenizer.texts_to_sequences(train_data['sentences'].values)\n",
    "x = pad_sequences(x)   \n",
    "x[:5]\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driving-individual",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words=500000\n",
    "tokenizer = Tokenizer(num_words= num_words,oov_token=\"unk\")\n",
    "tokenizer.fit_on_texts(train_data['sentences'].tolist())\n",
    "print(tokenizer.texts_to_sequences(train_data))\n",
    "#print(str(tokenizer.texts_to_sequences(['Dereje how are you'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patient-rogers",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words=500000\n",
    "tokenizer = Tokenizer(num_words= num_words,oov_token=\"unk\")\n",
    "tokenizer.fit_on_texts(train_data['sentences'].tolist())\n",
    "print(tokenizer.texts_to_sequences(train_data))\n",
    "#print(str(tokenizer.texts_to_sequences(['Dereje how are you'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sticky-knife",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.get_dummies(train_data['label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "falling-maldives",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.dropna(axis = 0, how ='any', inplace = True)\n",
    "train_data['Num_words_sentences'] = train_data['sentences'].apply(lambda x:len(str(x).split()))\n",
    "mask = train_data['Num_words_sentences']>2\n",
    "train_data = train_data[mask]\n",
    "print('++++++++++++++++Train data+++++++++++')\n",
    "print(train_data['sentences'])\n",
    "print(len(train_data))\n",
    "print('========================') \n",
    "max_train_sentence_length = train_data['Num_words_sentences'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unsigned-transcription",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_data = pd.read_csv(r'C:\\Users/HANU-TECH/Desktop/Data/test_data.csv')\n",
    "#dataset = datas.values.tolist()\n",
    "#print(dataset[0][0])\n",
    "test_data.dropna(axis =0, how ='any', inplace = True)\n",
    "test_data['Num_words_text'] = test_data['sentences'].apply(lambda x:len(str(x).split()))\n",
    "mask = test_data['Num_words_text']>2\n",
    "test_data = test_data[mask]\n",
    "print('++++++++++++++++Test data+++++++++++')\n",
    "print(test_data['label'])\n",
    "print(len(test_data))\n",
    "print('========================')\n",
    "max_test_sentence_length = test_data['Num_words_text'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liquid-banks",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.reshape(x, x.shape+(1,))\n",
    "Y = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "X.shape\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cutting-empty",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, Y_train, Y_valid = train_test_split(train_data['sentences'].tolist(),\\\n",
    "                                                     train_data['label'].tolist(),\\\n",
    "                                                     test_size = 0.1,\\\n",
    "                                                     stratify = train_data['label'].tolist(),\\\n",
    "                                                     random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorrect-lafayette",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train data len:'+str(len(X_train)))\n",
    "print('Class distribution:'+str(Counter(Y_train)))\n",
    "print('valid data len:'+str(len(X_valid)))\n",
    "print('Class distribution:'+str(Counter(Y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acquired-proceeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(tokenizer.texts_to_sequences(X_train))\n",
    "X_valid = np.array(tokenizer.texts_to_sequences(X_valid))\n",
    "X_test = np.array(tokenizer.texts_to_sequences(test_data['sentences'].tolist()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "israeli-offer",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(x_train, padding = 'post', maxlen =932)\n",
    "X_valid = pad_sequences(x_valid, padding = 'post', maxlen =40)\n",
    "X_test = pad_sequences(x_test, padding = 'post', maxlen =40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alleged-going",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "train_labels = le.fit_transform(y_train)\n",
    "train_labels = np.asarray(tf.keras.utils.to_categorical(train_labels))\n",
    "#print valid_labels\n",
    "valid_labels = le.transform(y_valid)\n",
    "valid_labels = np.asarray(tf.keras.utils.to_categorical(valid_labels))\n",
    "test_labels = le.transform(test_data['label'].tolist())\n",
    "test_labels = np.asarray(tf.keras.utils.to_categorical(test_labels))\n",
    "list(le.classes_)\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, train_labels))\n",
    "valid_ds = tf.data.Dataset.from_tensor_slices((x_valid, valid_labels))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, test_labels))\n",
    "print(y_train[:10])\n",
    "train_label = le.fit_transform(y_train)\n",
    "print(\"Text to number\")\n",
    "print(train_labels[:10])\n",
    "train_labels = np.asarray(tf.keras.utils.to_categorical(train_labels))\n",
    "print('Number to category')\n",
    "print(train_labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposed-field",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the model \n",
    "model = Sequential()\n",
    "model.add(Embedding(5000, 50,input_length = x.shape[1] ))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256, return_sequences = True, dropout = 0.3, recurrent_dropout = 0.2 ))\n",
    "model.add((LSTM(256, dropout = 0.3, recurrent_dropout = 0.2)))\n",
    "model.add(Dense(9, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designed-letter",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model Configuration \n",
    "model.compile(loss = tf.keras.losses.CategoricalCrossentropy(from_logits = True), optimizer ='adam' , metrics = ['CategoricalAccuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "veterinary-publication",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "history = model.fit(train_ds.shuffle(2000).batch(64),\n",
    "                   epochs = epochs,\n",
    "                   validation_data = valid_ds.batch(64),\n",
    "                   verbose =1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
